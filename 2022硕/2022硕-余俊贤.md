#  2021.12.3
* <font size=4>__*上午：*__</font>
> <font size=4>毕业实习焊板子；看完《Identifiability and predictability of integer- and fractional-order epidemiological models using physics-informed neural networks》第一页
* <font size=4>__*下午：*__</font>
>开组会。
* <font size=4>__*晚上：*__</font>
>反思这次组会汇报的像一锅浆糊的原因。看完《Identifiability and predictability of integer- and fractional-order epidemiological models using physics-informed neural networks》第二页和第三页半
* <font size=4>__*收获*__</font>
>1、这次组会让我意识到一个例子想要展示的前提是自己对它掌握的很到位。因为毕业实习的原因，这周论文学习的并不细致，对例题理解得一知半解，因此讲的时候支支吾吾，没有逻辑；
>2、这篇整数阶-分数阶流行病模型涉及到了SIR模型，首先学习了这个先验知识，大致了解了流行病学的背景知识，属于常微分问题，参数涉及时间依赖。

#  2021.12.2
* <font size=4>__*上午：*__</font>
> <font size=4>帮助辅导员审核各年级校长奖学金材料。
* <font size=4>__*下午：*__</font>
>制作校长奖学金汇总表，报送材料到学生工作处；看完泊肃叶NS方程的代码。
* <font size=4>__*晚上：*__</font>
>制作汇报PPT；继续挖掘GitHub的一些功能。
* <font size=4>__*收获*__</font>
>通过各种途径解决了GitHub不稳定的问题，已经可以流畅使用，大大减小了时间的浪费。这周零碎的杂事很多，明显感觉对论文的学习不够到位，制作PPT的时候知识的空缺感很明显。下周课内事情处理完以后重新调整状态，认真学习文献。


#  2021.12.1
* <font size=4>__*上午：*__</font>
> <font size=4>看完《Surrogate Modeling for fluid flows based on physics-constrained deep learning without simulation data》剩余内容。
* <font size=4>__*下午：*__</font>
>鼓楼区人大代表现场投票；进行毕业实习焊接充电板课程。
* <font size=4>__*晚上：*__</font>
>学习《Surrogate Modeling for fluid flows based on physics-constrained deep learning without simulation data》论文代码。
* <font size=4>__*收获*__</font>
>这篇文章和医学问题结合比较紧密，用动脉瘤对应的NS方程作为讨论对象，用他所提出的神经网络结构与传统解决这个问题的方法进行对比，分析结论。这个思路我觉得挺可取的，比如说师兄提到的变压器状态问题，之前的有限元方法完全可以作为对照组，方便我们对论文进行展开。这篇论文看到后面其实存在挺多问题，首先是所谓硬约束的适用范围，它只适用于第一类边界条件，即给定值，其次构造硬约束也很复杂，甚至还要专门用神经网络来训练，文章里一些硬约束的方程来的很突然，没有什么推导过程，让人很头秃。


#  2021.11.30
* <font size=4>__*上午：*__</font>
> <font size=4>完成电路仿真作业；按照要求格式修改每日汇报。</font>
* <font size=4>__*下午：*__</font>
>进行毕业实习焊接充电板课程；写毕业实习焊接心得部分内容。
* <font size=4>__*晚上：*__</font>
>阅读《Surrogate Modeling for fluid flows based on physics-constrained deep learning without simulation data》的第二节全部以及3.1内容。
* <font size=4>__*收获*__</font>
>这篇文章也比较有意思，他在文章和PINN做出了对比，把这种内嵌物理知识的方式定义为弱监督学习，同时他也把PINN里应用的损失函数构造方法定义为软约束，即把PDE边界初值条件都放入损失函数，但在准确性和解释上是难以确定的。在他自己的文章里，他使用了一种“硬约束”方法，根据边界条件和初始值重新构造神经网络的输出解，用这个新的解去构建损失函数——仅有PDE本身，但是他举的例子没太理解，要根据代码进一步学习。


#  2021.11.29
* <font size=4>__*上午：*__</font>
> <font size=4>学习戴老师发的git使用书籍和git desktop帖子。</font>
* <font size=4>__*下午：*__</font>
>粗略阅读两篇文章《data-driven discovery of partial differential equations》和《A unified deep artificial neural network approach to partial differential in complex geometries》。
* <font size=4>__*晚上：*__</font>
>批改党支部中级党课试卷。看《Surrogate Modeling for fluid flows based on physics-constrained deep learning without simulation data》的1，2.1内容。
* <font size=4>__*收获*__</font>
>最后看的那篇文章的思路和我之前想的很像，PINN的本质确实不是监督学习，这篇文章强调了一个词，“label-free”，不需要用别的方法求出数据，并且对最后一层输出做了处理，还没把握到本质，明天继续学习。




